# ============================================================================
# ğŸ”— Edge Service - CBAM ë°°ì¶œëŸ‰ ì „íŒŒ ì„œë¹„ìŠ¤
# ============================================================================

import logging
import os
from typing import Dict, List, Any, Optional, Tuple
from decimal import Decimal
from datetime import datetime, timezone
import asyncpg

logger = logging.getLogger(__name__)

class EdgeService:
    """ì—£ì§€ ê¸°ë°˜ ë°°ì¶œëŸ‰ ì „íŒŒ ì„œë¹„ìŠ¤ (asyncpg íŒ¨í„´)"""
    
    def __init__(self):
        self.database_url = os.getenv('DATABASE_URL')
        if not self.database_url:
            logger.warning("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            return
        
        self.pool = None
        self._initialization_attempted = False
        logger.info("âœ… Edge Service ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™”"""
        if self._initialization_attempted:
            return  # ì´ë¯¸ ì´ˆê¸°í™” ì‹œë„í–ˆìœ¼ë©´ ë‹¤ì‹œ ì‹œë„í•˜ì§€ ì•ŠìŒ
            
        if not self.database_url:
            logger.warning("DATABASE_URLì´ ì—†ì–´ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            self._initialization_attempted = True
            return
        
        self._initialization_attempted = True
        
        try:
            self.pool = await asyncpg.create_pool(
                self.database_url,
                min_size=1,
                max_size=10,
                command_timeout=30,
                server_settings={
                    'application_name': 'cbam-service-edge'
                }
            )
            logger.info("âœ… Edge Service ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ Edge Service ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            logger.warning("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ë¡œ ì¸í•´ ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            self.pool = None
    
    async def _ensure_pool_initialized(self):
        """ì—°ê²° í’€ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”ì‹œ ì´ˆê¸°í™”"""
        if not self.pool and not self._initialization_attempted:
            await self.initialize()
        
        if not self.pool:
            raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    async def get_process_emission_data(self, process_id: int) -> Optional[Dict[str, Any]]:
        """ê³µì •ì˜ ë°°ì¶œëŸ‰ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        await self._ensure_pool_initialized()
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT 
                        process_id,
                        attrdir_em,
                        cumulative_emission,
                        total_matdir_emission,
                        total_fueldir_emission
                    FROM process_attrdir_emission 
                    WHERE process_id = $1
                """
                row = await conn.fetchrow(query, process_id)
                
                if row:
                    return {
                        'process_id': row['process_id'],
                        'attrdir_em': float(row['attrdir_em']) if row['attrdir_em'] else 0.0,
                        'cumulative_emission': float(row['cumulative_emission']) if row['cumulative_emission'] else 0.0,
                        'total_matdir_emission': float(row['total_matdir_emission']) if row['total_matdir_emission'] else 0.0,
                        'total_fueldir_emission': float(row['total_fueldir_emission']) if row['total_fueldir_emission'] else 0.0
                    }
                return None
                
        except Exception as e:
            logger.error(f"ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def get_continue_edges(self, source_process_id: int) -> List[Dict[str, Any]]:
        """íŠ¹ì • ê³µì •ì—ì„œ ë‚˜ê°€ëŠ” continue ì—£ì§€ë“¤ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
        await self._ensure_pool_initialized()
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT 
                        id,
                        source_node_type,
                        source_id,
                        target_node_type,
                        target_id,
                        edge_kind
                    FROM edge 
                    WHERE source_node_type = 'process' 
                    AND source_id = $1 
                    AND edge_kind = 'continue'
                """
                rows = await conn.fetch(query, source_process_id)
                
                return [
                    {
                        'id': row['id'],
                        'source_node_type': row['source_node_type'],
                        'source_id': row['source_id'],
                        'target_node_type': row['target_node_type'],
                        'target_id': row['target_id'],
                        'edge_kind': row['edge_kind']
                    }
                    for row in rows
                ]
                
        except Exception as e:
            logger.error(f"ê³µì • {source_process_id}ì˜ continue ì—£ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def update_process_cumulative_emission(self, process_id: int, cumulative_emission: float) -> bool:
        """ê³µì •ì˜ ëˆ„ì  ë°°ì¶œëŸ‰ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        await self._ensure_pool_initialized()
        try:
            async with self.pool.acquire() as conn:
                query = """
                    UPDATE process_attrdir_emission 
                    SET 
                        cumulative_emission = $2,
                        updated_at = NOW()
                    WHERE process_id = $1
                """
                await conn.execute(query, process_id, cumulative_emission)
                
                logger.info(f"ê³µì • {process_id} ëˆ„ì  ë°°ì¶œëŸ‰ ì—…ë°ì´íŠ¸: {cumulative_emission}")
                return True
                
        except Exception as e:
            logger.error(f"ê³µì • {process_id} ëˆ„ì  ë°°ì¶œëŸ‰ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def propagate_emissions_continue(self, source_process_id: int, target_process_id: int) -> bool:
        """
        ê·œì¹™ 1: ê³µì •â†’ê³µì • ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ (edge_kind = "continue")
        source.attr_emì´ targetìœ¼ë¡œ ëˆ„ì  ì „ë‹¬ë˜ì–´ target.cumulative_emission = source.cumulative_emission + target.attrdir_em
        """
        try:
            logger.info(f"ğŸ”— ê³µì • {source_process_id} â†’ ê³µì • {target_process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹œì‘")
            
            # 1. ì†ŒìŠ¤ ê³µì •ì˜ ëˆ„ì  ë°°ì¶œëŸ‰ ì¡°íšŒ
            source_emission = await self.get_process_emission_data(source_process_id)
            if not source_emission:
                logger.error(f"ì†ŒìŠ¤ ê³µì • {source_process_id}ì˜ ë°°ì¶œëŸ‰ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 2. íƒ€ê²Ÿ ê³µì •ì˜ ë°°ì¶œëŸ‰ ë°ì´í„° ì¡°íšŒ
            target_emission = await self.get_process_emission_data(target_process_id)
            if not target_emission:
                logger.error(f"íƒ€ê²Ÿ ê³µì • {target_process_id}ì˜ ë°°ì¶œëŸ‰ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 3. ë°°ì¶œëŸ‰ ëˆ„ì  ê³„ì‚°
            source_cumulative = source_emission['cumulative_emission']
            target_own = target_emission['attrdir_em']
            target_cumulative = source_cumulative + target_own
            
            logger.info(f"ğŸ§® ë°°ì¶œëŸ‰ ëˆ„ì  ê³„ì‚°:")
            logger.info(f"  ì†ŒìŠ¤ ê³µì • {source_process_id} ëˆ„ì  ë°°ì¶œëŸ‰: {source_cumulative}")
            logger.info(f"  íƒ€ê²Ÿ ê³µì • {target_process_id} ìì²´ ë°°ì¶œëŸ‰: {target_own}")
            logger.info(f"  íƒ€ê²Ÿ ê³µì • {target_process_id} ìµœì¢… ëˆ„ì  ë°°ì¶œëŸ‰: {target_cumulative}")
            
            # 4. íƒ€ê²Ÿ ê³µì •ì˜ ëˆ„ì  ë°°ì¶œëŸ‰ ì—…ë°ì´íŠ¸
            success = await self.update_process_cumulative_emission(target_process_id, target_cumulative)
            
            if success:
                logger.info(f"âœ… ê³µì • {source_process_id} â†’ ê³µì • {target_process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì™„ë£Œ")
                return True
            else:
                logger.error(f"âŒ ê³µì • {target_process_id} ëˆ„ì  ë°°ì¶œëŸ‰ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"ê³µì • {source_process_id} â†’ ê³µì • {target_process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹¤íŒ¨: {e}")
            return False
    
    async def propagate_emissions_chain(self, process_chain_id: int) -> Dict[str, Any]:
        """
        ê³µì • ì²´ì¸ ì „ì²´ì— ëŒ€í•´ ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        try:
            logger.info(f"ğŸ”— ê³µì • ì²´ì¸ {process_chain_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹œì‘")
            
            # 1. ê³µì • ì²´ì¸ì˜ ìˆœì„œ ì •ë³´ ì¡°íšŒ
            chain_query = text("""
                SELECT pcl.process_id, pcl.sequence_order, pcl.is_continue_edge
                FROM process_chain_link pcl
                WHERE pcl.chain_id = :chain_id
                ORDER BY pcl.sequence_order
            """)
            
            result = await self.db_session.execute(chain_query, {'chain_id': process_chain_id})
            chain_processes = result.fetchall()
            
            if not chain_processes:
                logger.error(f"ê³µì • ì²´ì¸ {process_chain_id}ì˜ ê³µì • ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {'success': False, 'error': 'ê³µì • ì²´ì¸ ì •ë³´ ì—†ìŒ'}
            
            logger.info(f"ğŸ“‹ ê³µì • ì²´ì¸ {process_chain_id} ê³µì • ìˆœì„œ: {len(chain_processes)}ê°œ")
            
            # 2. ìˆœì„œëŒ€ë¡œ ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹¤í–‰
            propagation_results = []
            previous_process_id = None
            
            for i, (process_id, sequence_order, is_continue_edge) in enumerate(chain_processes):
                logger.info(f"ğŸ” ê³µì • {process_id} (ìˆœì„œ: {sequence_order}) ì²˜ë¦¬ ì¤‘...")
                
                if i == 0:
                    # ì²« ë²ˆì§¸ ê³µì •: ëˆ„ì  ë°°ì¶œëŸ‰ = ìì²´ ë°°ì¶œëŸ‰
                    emission_data = await self.get_process_emission_data(process_id)
                    if emission_data:
                        own_emission = emission_data['attrdir_em']
                        success = await self.update_process_cumulative_emission(process_id, own_emission)
                        
                        propagation_results.append({
                            'process_id': process_id,
                            'sequence_order': sequence_order,
                            'own_emission': own_emission,
                            'cumulative_emission': own_emission,
                            'propagation_type': 'first_process',
                            'success': success
                        })
                        
                        previous_process_id = process_id
                        logger.info(f"âœ… ì²« ë²ˆì§¸ ê³µì • {process_id} ëˆ„ì  ë°°ì¶œëŸ‰ ì„¤ì •: {own_emission}")
                    else:
                        logger.error(f"ì²« ë²ˆì§¸ ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì—†ìŒ")
                        return {'success': False, 'error': f'ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì—†ìŒ'}
                        
                elif is_continue_edge and previous_process_id:
                    # continue ì—£ì§€ê°€ ìˆëŠ” ê²½ìš°: ì´ì „ ê³µì •ì—ì„œ ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬
                    success = await self.propagate_emissions_continue(previous_process_id, process_id)
                    
                    if success:
                        # ì—…ë°ì´íŠ¸ëœ ëˆ„ì  ë°°ì¶œëŸ‰ ì¡°íšŒ
                        updated_emission = await self.get_process_emission_data(process_id)
                        if updated_emission:
                            propagation_results.append({
                                'process_id': process_id,
                                'sequence_order': sequence_order,
                                'own_emission': updated_emission['attrdir_em'],
                                'cumulative_emission': updated_emission['cumulative_emission'],
                                'propagation_type': 'continue_edge',
                                'source_process_id': previous_process_id,
                                'success': True
                            })
                            
                            previous_process_id = process_id
                            logger.info(f"âœ… ê³µì • {process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì™„ë£Œ")
                        else:
                            logger.error(f"ê³µì • {process_id} ì—…ë°ì´íŠ¸ëœ ë°°ì¶œëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨")
                            return {'success': False, 'error': f'ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨'}
                    else:
                        logger.error(f"ê³µì • {previous_process_id} â†’ ê³µì • {process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹¤íŒ¨")
                        return {'success': False, 'error': f'ê³µì • {process_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹¤íŒ¨'}
                        
                else:
                    # continue ì—£ì§€ê°€ ì—†ëŠ” ê²½ìš°: ìì²´ ë°°ì¶œëŸ‰ë§Œ ì„¤ì •
                    emission_data = await self.get_process_emission_data(process_id)
                    if emission_data:
                        own_emission = emission_data['attrdir_em']
                        success = await self.update_process_cumulative_emission(process_id, own_emission)
                        
                        propagation_results.append({
                            'process_id': process_id,
                            'sequence_order': sequence_order,
                            'own_emission': own_emission,
                            'cumulative_emission': own_emission,
                            'propagation_type': 'no_continue_edge',
                            'success': success
                        })
                        
                        previous_process_id = process_id
                        logger.info(f"âœ… ê³µì • {process_id} ìì²´ ë°°ì¶œëŸ‰ë§Œ ì„¤ì •: {own_emission}")
                    else:
                        logger.error(f"ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì—†ìŒ")
                        return {'success': False, 'error': f'ê³µì • {process_id} ë°°ì¶œëŸ‰ ë°ì´í„° ì—†ìŒ'}
            
            # 3. ê²°ê³¼ ìš”ì•½
            total_processes = len(propagation_results)
            successful_propagations = len([r for r in propagation_results if r['success']])
            
            final_result = {
                'success': True,
                'chain_id': process_chain_id,
                'total_processes': total_processes,
                'successful_propagations': successful_propagations,
                'propagation_results': propagation_results,
                'final_emission_summary': {
                    'total_own_emissions': sum(r['own_emission'] for r in propagation_results),
                    'total_cumulative_emissions': sum(r['cumulative_emission'] for r in propagation_results),
                    'last_process_cumulative': propagation_results[-1]['cumulative_emission'] if propagation_results else 0
                }
            }
            
            logger.info(f"ğŸ¯ ê³µì • ì²´ì¸ {process_chain_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì™„ë£Œ!")
            logger.info(f"  ì´ ê³µì • ìˆ˜: {total_processes}")
            logger.info(f"  ì„±ê³µí•œ ì „íŒŒ: {successful_propagations}")
            logger.info(f"  ìµœì¢… ëˆ„ì  ë°°ì¶œëŸ‰: {final_result['final_emission_summary']['last_process_cumulative']}")
            
            return final_result
            
        except Exception as e:
            logger.error(f"ê³µì • ì²´ì¸ {process_chain_id} ë°°ì¶œëŸ‰ ëˆ„ì  ì „ë‹¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def get_process_chain_emission_summary(self, process_chain_id: int) -> Dict[str, Any]:
        """ê³µì • ì²´ì¸ì˜ ë°°ì¶œëŸ‰ ìš”ì•½ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
        try:
            summary_query = text("""
                SELECT 
                    pcl.sequence_order,
                    pcl.process_id,
                    p.process_name,
                    pae.attrdir_em,
                    pae.cumulative_emission,
                    pae.calculation_date
                FROM process_chain_link pcl
                JOIN process p ON pcl.process_id = p.id
                LEFT JOIN process_attrdir_emission pae ON pcl.process_id = pae.process_id
                WHERE pcl.chain_id = :chain_id
                ORDER BY pcl.sequence_order
            """)
            
            result = await self.db_session.execute(summary_query, {'chain_id': process_chain_id})
            processes = result.fetchall()
            
            if not processes:
                return {'success': False, 'error': 'ê³µì • ì²´ì¸ ì •ë³´ ì—†ìŒ'}
            
            summary = {
                'chain_id': process_chain_id,
                'total_processes': len(processes),
                'processes': [
                    {
                        'sequence_order': proc.sequence_order,
                        'process_id': proc.process_id,
                        'process_name': proc.process_name,
                        'own_emission': float(proc.attrdir_em) if proc.attrdir_em else 0.0,
                        'cumulative_emission': float(proc.cumulative_emission) if proc.cumulative_emission else 0.0,
                        'calculation_date': proc.calculation_date.isoformat() if proc.calculation_date else None
                    }
                    for proc in processes
                ],
                'total_own_emissions': sum(float(proc.attrdir_em) if proc.attrdir_em else 0.0 for proc in processes),
                'total_cumulative_emissions': sum(float(proc.cumulative_emission) if proc.cumulative_emission else 0.0 for proc in processes)
            }
            
            return {'success': True, 'summary': summary}
            
        except Exception as e:
            logger.error(f"ê³µì • ì²´ì¸ {process_chain_id} ë°°ì¶œëŸ‰ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}

    # ============================================================================
    # ğŸ”— ê¸°ì¡´ Edge CRUD ë©”ì„œë“œë“¤
    # ============================================================================
    
    async def create_edge(self, edge_data) -> Optional[Dict[str, Any]]:
        """ì—£ì§€ ìƒì„± (asyncpg íŒ¨í„´)"""
        await self._ensure_pool_initialized()
        try:
            logger.info(f"ì—£ì§€ ìƒì„± ì‹œì‘: {edge_data}")
            
            async with self.pool.acquire() as conn:
                query = """
                    INSERT INTO edge (source_node_type, source_id, target_node_type, target_id, edge_kind, created_at, updated_at)
                    VALUES ($1, $2, $3, $4, $5, $6, $6)
                    RETURNING id, source_node_type, source_id, target_node_type, target_id, edge_kind, created_at, updated_at
                """
                
                now = datetime.now(timezone.utc)
                row = await conn.fetchrow(
                    query,
                    edge_data.source_node_type,
                    edge_data.source_id,
                    edge_data.target_node_type,
                    edge_data.target_id,
                    edge_data.edge_kind,
                    now
                )
                
                if row:
                    result = {
                        'id': row['id'],
                        'source_node_type': row['source_node_type'],
                        'source_id': row['source_id'],
                        'target_node_type': row['target_node_type'],
                        'target_id': row['target_id'],
                        'edge_kind': row['edge_kind'],
                        'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                        'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None
                    }
                    logger.info(f"âœ… ì—£ì§€ ìƒì„± ì™„ë£Œ: ID {row['id']}")
                    return result
                else:
                    logger.error("ì—£ì§€ ìƒì„± ì‹¤íŒ¨: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return None
                    
        except Exception as e:
            logger.error(f"ì—£ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
            raise e
    
    async def get_edges(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì—£ì§€ ì¡°íšŒ (asyncpg íŒ¨í„´)"""
        await self._ensure_pool_initialized()
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT id, source_node_type, source_id, target_node_type, target_id, edge_kind, created_at, updated_at
                    FROM edge
                    ORDER BY id
                """
                rows = await conn.fetch(query)
                
                return [
                    {
                        'id': row['id'],
                        'source_node_type': row['source_node_type'],
                        'source_id': row['source_id'],
                        'target_node_type': row['target_node_type'],
                        'target_id': row['target_id'],
                        'edge_kind': row['edge_kind'],
                        'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                        'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None
                    }
                    for row in rows
                ]
        except Exception as e:
            logger.error(f"ì—£ì§€ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def get_edge(self, edge_id: int) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ì—£ì§€ ì¡°íšŒ (asyncpg íŒ¨í„´)"""
        await self._ensure_pool_initialized()
        try:
            async with self.pool.acquire() as conn:
                query = """
                    SELECT id, source_node_type, source_id, target_node_type, target_id, edge_kind, created_at, updated_at
                    FROM edge
                    WHERE id = $1
                """
                row = await conn.fetchrow(query, edge_id)
                
                if row:
                    return {
                        'id': row['id'],
                        'source_node_type': row['source_node_type'],
                        'source_id': row['source_id'],
                        'target_node_type': row['target_node_type'],
                        'target_id': row['target_id'],
                        'edge_kind': row['edge_kind'],
                        'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                        'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None
                    }
                return None
        except Exception as e:
            logger.error(f"ì—£ì§€ {edge_id} ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def update_edge(self, edge_id: int, edge_data) -> Optional[Dict[str, Any]]:
        """ì—£ì§€ ìˆ˜ì • (asyncpg íŒ¨í„´)"""
        await self._ensure_pool_initialized()
        try:
            logger.info(f"ì—£ì§€ {edge_id} ìˆ˜ì •: {edge_data}")
            
            # ê¸°ì¡´ ì—£ì§€ ì¡°íšŒ
            existing_edge = await self.get_edge(edge_id)
            if not existing_edge:
                return None
            
            async with self.pool.acquire() as conn:
                # ì—…ë°ì´íŠ¸í•  í•„ë“œë“¤ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±
                update_fields = []
                params = []
                param_count = 1
                
                if edge_data.source_node_type is not None:
                    update_fields.append(f"source_node_type = ${param_count}")
                    params.append(edge_data.source_node_type)
                    param_count += 1
                
                if edge_data.source_id is not None:
                    update_fields.append(f"source_id = ${param_count}")
                    params.append(edge_data.source_id)
                    param_count += 1
                
                if edge_data.target_node_type is not None:
                    update_fields.append(f"target_node_type = ${param_count}")
                    params.append(edge_data.target_node_type)
                    param_count += 1
                
                if edge_data.target_id is not None:
                    update_fields.append(f"target_id = ${param_count}")
                    params.append(edge_data.target_id)
                    param_count += 1
                
                if edge_data.edge_kind is not None:
                    update_fields.append(f"edge_kind = ${param_count}")
                    params.append(edge_data.edge_kind)
                    param_count += 1
                
                # updated_at í•„ë“œ ì¶”ê°€
                update_fields.append(f"updated_at = ${param_count}")
                params.append(datetime.now(timezone.utc))
                param_count += 1
                
                # ID íŒŒë¼ë¯¸í„° ì¶”ê°€
                params.append(edge_id)
                
                if update_fields:
                    query = f"""
                        UPDATE edge 
                        SET {', '.join(update_fields)}
                        WHERE id = ${param_count}
                        RETURNING id, source_node_type, source_id, target_node_type, target_id, edge_kind, created_at, updated_at
                    """
                    
                    row = await conn.fetchrow(query, *params)
                    
                    if row:
                        result = {
                            'id': row['id'],
                            'source_node_type': row['source_node_type'],
                            'source_id': row['source_id'],
                            'target_node_type': row['target_node_type'],
                            'target_id': row['target_id'],
                            'edge_kind': row['edge_kind'],
                            'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                            'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None
                        }
                        logger.info(f"âœ… ì—£ì§€ {edge_id} ìˆ˜ì • ì™„ë£Œ")
                        return result
                
                return None
                
        except Exception as e:
            logger.error(f"ì—£ì§€ {edge_id} ìˆ˜ì • ì‹¤íŒ¨: {e}")
            raise e
    
    async def delete_edge(self, edge_id: int) -> bool:
        """ì—£ì§€ ì‚­ì œ (asyncpg íŒ¨í„´)"""
        await self._ensure_pool_initialized()
        try:
            logger.info(f"ì—£ì§€ {edge_id} ì‚­ì œ")
            
            # ê¸°ì¡´ ì—£ì§€ ì¡°íšŒ
            existing_edge = await self.get_edge(edge_id)
            if not existing_edge:
                return False
            
            async with self.pool.acquire() as conn:
                query = """
                    DELETE FROM edge 
                    WHERE id = $1
                    RETURNING id
                """
                
                row = await conn.fetchrow(query, edge_id)
                
                if row:
                    logger.info(f"âœ… ì—£ì§€ {edge_id} ì‚­ì œ ì™„ë£Œ")
                    return True
                else:
                    logger.warning(f"ì—£ì§€ {edge_id} ì‚­ì œ ì‹¤íŒ¨: í•´ë‹¹ ì—£ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return False
                    
        except Exception as e:
            logger.error(f"ì—£ì§€ {edge_id} ì‚­ì œ ì‹¤íŒ¨: {e}")
            raise e
